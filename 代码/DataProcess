{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataProcess","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1Trt0mPYBM5OZu5V1dmf5-THZLZkHosoa","authorship_tag":"ABX9TyP/0RrIYzBFz64bB4PnnqTw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"2XyL0djNvPCp","colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"status":"error","timestamp":1619230653344,"user_tz":-480,"elapsed":1366,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"45db3fc0-b8cd-49cd-fc60-4873aa552fc6"},"source":["#### 1.generate file\n","\n","import pandas as pd\n","\n","file = pd.read_table('/content/drive/My Drive/Data/routes.dat',sep=',',header=None)\n","file.columns = ['1','2','SourceAirportID','3','DestinationAirportID','4','5','6','7']\n","df = file.drop(columns=['1','2','3','4','5','6','7'])\n","df['HasAirline']=1\n","\n","#-----提取-----\n","airport_set = set()\n","line_set = set()\n","for i in range(len(df)):\n","    airport1 = df.loc[i,'SourceAirportID']\n","    airport2 = df.loc[i,'DestinationAirportID']\n","    airport_set.add(airport1)\n","    airport_set.add(airport2)\n","    line_set.add(airport1 + '#' + airport2)\n","#   line_set.add(airport2 + '#' + airport1)\n","\n","#-----反例----- \n","neg_set = set()\n","for  airport1 in airport_set:\n","    for airport2 in airport_set:\n","        pair1 = airport1 + '#' + airport2\n","        pair2 = airport2 + '#' + airport1\n","        if pair1 not in line_set:\n","            neg_set.add(pair1)\n","        if pair2 not in line_set:\n","            neg_set.add(pair2)\n","print(len(neg_set))\n","\n","#-----权重-----\n","weight_dict = dict.fromkeys(line_set,0)\n","for i in range(len(df)):\n","    airport1 = df.loc[i,'SourceAirportID']\n","    airport2 = df.loc[i,'DestinationAirportID']\n","    weight_dict[airport1 + '#' + airport2] += 1\n","\n","#-----写入-----\n","def f_write(data, name):\n","  data = str(data)\n","  f = open('/content/drive/My Drive/Data_Ready/' + name + '.txt','w')\n","  f.writelines(data)\n","  f.close()\n","\n","def f_sort_write(list_data, name):\n","  list_train_list = list_data[0:round(len(list_data) * 0.8)]\n","  list_test_list = list_data[round(len(list_data) * 0.8):round(len(list_data) * 0.9)] \n","  list_verify_list = list_data[round(len(list_data) * 0.9):round(len(list_data))]\n","  f_write(list_train_list, name + '_train_list')\n","  f_write(list_test_list, name + '_test_list')\n","  f_write(list_verify_list, name + '_verify_list')                                                                \n","                                                              \n","\n","#f_write(weight_dict,'weight_dict')\n","\n","line_list = list(line_set)\n","f_sort_write(line_list, 'line')\n","\n","airport_train_set = set()\n","for i in range(37595):\n","  airport_train_set.add(line_list[i].split('#')[0])\n","  airport_train_set.add(line_list[i].split('#')[1])\n","\n","i = 0\n","j = 0\n","temp = list(neg_set)\n","neg_list = [0 for x in range(37595)]\n","while i < len(line_list):\n","  neg_list[i] = temp[j] \n","  if (neg_list[i].split('#')[0] not in airport_train_set) | (neg_list[i].split('#')[1] not in airport_train_set):\n","    continue\n","  i += 1\n","  j += 100\n","f_sort_write(neg_list, 'neg')\n","\n","print(len(neg_list))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-78469ec4aa89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Data/routes.dat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SourceAirportID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DestinationAirportID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'6'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'7'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'6'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'7'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;31m# default to avoid a ValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Data/routes.dat'"]}]},{"cell_type":"code","metadata":{"id":"2jlfIUkiYTOQ","colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"status":"error","timestamp":1619230761990,"user_tz":-480,"elapsed":2040,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"8223aed9-ae93-46cc-b73f-bc3a0c9d9f61"},"source":["#### 2.generate word2vec model\n","\n","import networkx as nx\n","import pandas as pd\n","from node2vec import Node2Vec\n","import ast\n","import matplotlib.pyplot as plt \n","\n","#### make line_data to set\n","with open('/content/drive/My Drive/Data_Ok/line_train_list.txt') as f:\n","  line_train_data = f.read()\n","line_train_list = eval(line_train_data)\n","line_train_set = set(line_train_list)\n","\n","with open('/content/drive/My Drive/Data_Ok/line_data.txt') as f:\n","  line_data = f.read()\n","line_set = set(eval(line_data))\n","\n","#### recover weight_list to weight_dic, ast.literal_eval can check the\n","#### list inorder not to run bad code\n","with open('/content/drive/My Drive/Data_Ok/weight_dict.txt') as weight_file:\n","  weight_list = weight_file.readline()\n","weight_dict = ast.literal_eval(weight_list)\n","\n","#### generate graph\n","G = nx.Graph()\n","weight_max = max(weight_dict.values())\n","i = 0\n","\n","for pair in line_train_set:\n","  i+=1\n","  G.add_edge(pair.split('#')[0], pair.split('#')[1], weight = weight_dict[pair]/weight_max)\n","\n","for pair in line_set:\n","  G.add_node(pair.split('#')[0])\n","  G.add_node(pair.split('#')[1])\n","\n","#### generate embedding\n","def devide_pq(G,p,q):\n","  node2vec = Node2Vec(G, dimensions=64, walk_length = 10, num_walks = 80, p = p, q = q, workers = 1)\n","  model = node2vec.fit(window=10, min_count=1, batch_words=4)\n","  model.save('/content/drive/My Drive/Result/model'+ str(p) + str(q))  \n","  print('save success' + str(p) + str(q))\n","\n","print(len(G.degree))\n","##devide_pq(G,1/4,4)\n","'''\n","devide_pq(G,1/16,16)\n","devide_pq(G,1/8,8)\n","\n","devide_pq(G,1/2,2)\n","devide_pq(G,1,1)\n","devide_pq(G,2,1/2)\n","devide_pq(G,4,1/4)\n","devide_pq(G,8,1/8)\n","devide_pq(G,16,1/16)\n","##nx.draw(G,pos = nx.random_layout(G),node_color = 'b',edge_color = 'r',with_labels = False,font_size =1,node_size =2)\n","\n","##plt.show()\n","''' "],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-eff81de4228c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnode2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'node2vec'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"2mHYqo-Ni7GS","colab":{"base_uri":"https://localhost:8080/","height":476},"executionInfo":{"status":"ok","timestamp":1590923647136,"user_tz":-480,"elapsed":6507,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"68a2c836-705e-4f37-9d9f-8c57e3df8642"},"source":["pip install node2vec\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting node2vec\n","  Downloading https://files.pythonhosted.org/packages/c0/da/7f0c49433ef91033e21d523e82be1570074a5d6ab8c74f8771774e9d2fd1/node2vec-0.3.2-py3-none-any.whl\n","Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from node2vec) (3.6.0)\n","Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.6/dist-packages (from node2vec) (0.15.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from node2vec) (4.41.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from node2vec) (2.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from node2vec) (1.18.4)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->node2vec) (1.12.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->node2vec) (2.0.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->node2vec) (1.4.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->node2vec) (4.4.2)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->node2vec) (1.13.13)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->node2vec) (2.23.0)\n","Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->node2vec) (2.49.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->node2vec) (0.10.0)\n","Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->node2vec) (1.16.13)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->node2vec) (0.3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->node2vec) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->node2vec) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->node2vec) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->node2vec) (2.9)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim->node2vec) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim->node2vec) (2.8.1)\n","Installing collected packages: node2vec\n","Successfully installed node2vec-0.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OZu9p9brMz88","colab":{"base_uri":"https://localhost:8080/","height":253},"executionInfo":{"status":"ok","timestamp":1619231901220,"user_tz":-480,"elapsed":526225,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"9d694a56-0c0b-43e9-95c7-46af1e60fb05"},"source":["#### machine learning\n","\n","from gensim.models import word2vec\n","import numpy as np\n","from sklearn.metrics import recall_score, precision_score, f1_score\n","from sklearn.linear_model import LogisticRegression \n","from sklearn.linear_model import RidgeCV\n","import sklearn.svm as svm\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","#### use read to get strings, then use eval to recover them\n","def read_list(name, kind):\n","  with open('/content/drive/My Drive/Data_Ok/' + name + '_' + kind + '_list.txt') as f:\n","    name_kind = f.read()\n","  return eval(name_kind)\n","\n","line_train_list = read_list('line', 'train')\n","neg_train_list = read_list('neg', 'train')\n","line_test_list = read_list('line', 'test')\n","neg_test_list = read_list('neg', 'test')\n","\n","model = 'model2'\n","w2v_model = word2vec.Word2Vec.load('/content/drive/My Drive/Result/' + model)\n","dimentions = 64\n","\n","'''\n","print(w2v_model['CFB'])\n","print(w2v_model['PLU'])\n","print(w2v_model['SJK'])\n","'''\n","train_x = np.zeros((len(line_train_list) * 2, dimentions * 2))\n","train_y = np.zeros((len(line_train_list) * 2, 1), dtype = int)\n","test_x = np.zeros((len(line_test_list) * 2, dimentions * 2))\n","test_y = np.zeros((len(line_test_list) * 2, 1), dtype = int)\n","\n","def generate_xy(line_list, neg_list, x, y):\n","  i = 0\n","  for pair in line_list:\n","    x[i,0:dimentions] = w2v_model[pair.split('#')[0]]\n","    x[i,dimentions:dimentions * 2] = w2v_model[pair.split('#')[1]]\n","    y[i,0] = 1\n","    i += 1\n","  for pair in neg_list:\n","    x[i,0:dimentions] = w2v_model[pair.split('#')[0]]\n","    x[i,dimentions:dimentions * 2] = w2v_model[pair.split('#')[1]]\n","    y[i,0] = 0\n","    i += 1\n","  return x, y\n","\n","train_x, train_y = generate_xy(line_train_list, neg_train_list, train_x, train_y)\n","test_x, test_y = generate_xy(line_test_list, neg_test_list, test_x, test_y)\n","\n","def evaluation(x, y):\n","  print('p:', precision_score(x,y),'r:', recall_score(x,y), 'f1:', f1_score(x,y))\n","'''  \n","#### LR\n","LR_model = LogisticRegression()\n","LR_model.fit(train_x, train_y)\n","evaluation(test_y, LR_model.predict(test_x))\n","'''\n","#### svm\n","svm_model = svm.SVC(kernel='rbf', C=600)\n","svm_model.fit(train_x, train_y)\n","#svm_score = accuracy_score(test_y, svm_model.predict(test_x))\n","evaluation(test_y, svm_model.predict(test_x))\n","\n","'''\n","#### MLPClassifier\n","MLP_model = MLPClassifier(hidden_layer_sizes=(500, ), activation='relu', \n","    solver='lbfgs', alpha=0.0001, batch_size='auto', \n","    learning_rate='constant')\n","MLP_model.fit(train_x, train_y)\n","#MLP_score = accuracy_score(test_y, MLP_model.predict(test_x))\n","evaluation(test_y, MLP_model.predict(test_x))\n","'''\n","'''\n","#### RandomForestClassifier\n","RF_model = RandomForestClassifier()\n","RF_model.fit(train_x, train_y)\n","RF_score = accuracy_score(test_y, RF_model.predict(test_x))\n","\n","#### RidgeCV\n","Ridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0])\n","Ridge_model.fit(train_x, train_y)\n","Ridge_score = accuracy_score(test_y, np.rint(Ridge_model.predict(test_x)))\n","\n","#### DecisionTreeClassifier\n","DT_model = DecisionTreeClassifier()\n","DT_model.fit(train_x, train_y)\n","DT_score = accuracy_score(test_y, DT_model.predict(test_x))\n","\n","print('各种模型预测正确率')\n","print('LR:', LR_score)\n","print('RidgeCV:', Ridge_score)\n","print('svm:', svm_score)\n","print('MLPClassifier:', MLP_score)\n","print('RandomForestClassifier:', RF_score)\n","print('DecisionTreeClassifier:', DT_score)\n","'''"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"stream","text":["p: 0.9798639455782313 r: 0.9577127659574468 f1: 0.9686617350369873\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n#### RandomForestClassifier\\nRF_model = RandomForestClassifier()\\nRF_model.fit(train_x, train_y)\\nRF_score = accuracy_score(test_y, RF_model.predict(test_x))\\n\\n#### RidgeCV\\nRidge_model = RidgeCV(alphas=[0.1, 1.0, 10.0])\\nRidge_model.fit(train_x, train_y)\\nRidge_score = accuracy_score(test_y, np.rint(Ridge_model.predict(test_x)))\\n\\n#### DecisionTreeClassifier\\nDT_model = DecisionTreeClassifier()\\nDT_model.fit(train_x, train_y)\\nDT_score = accuracy_score(test_y, DT_model.predict(test_x))\\n\\nprint('各种模型预测正确率')\\nprint('LR:', LR_score)\\nprint('RidgeCV:', Ridge_score)\\nprint('svm:', svm_score)\\nprint('MLPClassifier:', MLP_score)\\nprint('RandomForestClassifier:', RF_score)\\nprint('DecisionTreeClassifier:', DT_score)\\n\""]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"g_pES7vVrk55"},"source":["# 新段落"]},{"cell_type":"code","metadata":{"id":"_X-9XH33gxFI","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1587468107346,"user_tz":-480,"elapsed":188092,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"ad060e00-09c7-4bc7-e389-f920a8d127e6"},"source":["#### 2.deepwalk: generate word2vec model2\n","\n","import networkx as nx\n","import pandas as pd\n","import ast\n","import gensim.models.word2vec as w2v\n","import numpy as np\n","import random\n","\n","#### make line_data to set\n","with open('/content/drive/My Drive/Data_Ok/line_train_list.txt') as f:\n","  line_train_data = f.read()\n","line_train_list = eval(line_train_data)\n","line_train_set = set(line_train_list)\n","\n","with open('/content/drive/My Drive/Data_Ok/line_data.txt') as f:\n","  line_data = f.read()\n","line_set = set(eval(line_data))\n","\n","#### recover weight_list to weight_dic, ast.literal_eval can check the\n","#### list inorder not to run bad code\n","with open('/content/drive/My Drive/Data_Ok/weight_dict.txt') as weight_file:\n","  weight_list = weight_file.readline()\n","weight_dict = ast.literal_eval(weight_list)\n","\n","#### generate graph\n","G = nx.Graph()\n","airport_set = set()\n","weight_max = max(weight_dict.values())\n","for pair in line_train_set:\n","  G.add_edge(pair.split('#')[0], pair.split('#')[1], weight = weight_dict[pair]/weight_max)\n","for pair in line_set:\n","  G.add_node(pair.split('#')[0])\n","  airport_set.add(pair.split('#')[0])\n","  G.add_node(pair.split('#')[1])\n","  airport_set.add(pair.split('#')[1])\n","\n","#nx.draw(G)\n","\n","def randomWalk(G, corpus_num, deep_num, given_word):\n","  corpus = []\n","  for i in range(corpus_num):\n","    sentence = [given_word]\n","    current_word = given_word\n","    count = 0\n","    while count < deep_num:\n","      count += 1\n","      node_list = []\n","      weight_list = []\n","      for nbr, data in G[current_word].items():\n","        node_list.append(nbr)\n","        weight_list.append(data['weight'])\n","      count2 = 0\n","      #print(int(weight_list[1] * weight_max))\n","      #print(len(node_list),weight_list)\n","      while count2 < len(weight_list):\n","        #print(count2)\n","        #print(int(weight_list[count2] * weight_max),count2,weight_list,weight_max)\n","        length = int(weight_list[count2] * weight_max)\n","        for weight_num in range(length):\n","          node_list.append(node_list[count2])\n","        count2 += 1\n","      if len(node_list) == 1:\n","        num = 0\n","      elif len(node_list) == 0:\n","        break\n","      else:\n","        num = random.randint(0, len(node_list) - 1)\n","      sel_node = node_list[num]\n","      sentence.append(sel_node)\n","      current_word = sel_node\n","    corpus.append(sentence)\n","  return corpus\n","  \n","num_walks = 80\n","walk_length = 10\n","sentences = []\n","for airport in airport_set:\n","  corpus = randomWalk(G, num_walks, walk_length, airport)\n","  for cols in corpus:\n","    sentences.append(cols)\n","\n","print(sentences[0:5])\n","#model = w2v.Word2Vec(sentences, size = 64, window = 10)\n","#model.save('/content/drive/My Drive/Result/model2')  \n","print('save success')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['CFB', 'PLU', 'SJK', 'PLU', 'RAO', 'UDI', 'PLU', 'RAO', 'CGH', 'SJP', 'RAO'], ['CFB', 'SDU', 'SJK', 'SDU', 'SJK', 'SDU', 'CFB', 'VCP', 'BSB', 'RBR', 'CZS'], ['CFB', 'VCP', 'GYN', 'GIG', 'MAO', 'STM', 'MEU', 'STM', 'MAO', 'PIN', 'MAO'], ['CFB', 'SDU', 'VIX', 'BSB', 'CGR', 'CGB', 'GYN', 'GIG', 'BEL', 'MAO', 'GRU'], ['CFB', 'SDU', 'CWB', 'LDB', 'GRU', 'CNF', 'MOC', 'CNF', 'BSB', 'RAO', 'BSB']]\n","save success\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L1UDObh9b2mU","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1587468115504,"user_tz":-480,"elapsed":196104,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"d91b4395-8898-413d-b673-a5b786fcc4d1"},"source":["#### link prediction\n","\n","import networkx as nx\n","import pandas as pd\n","import ast\n","\n","#### make line_data to set\n","with open('/content/drive/My Drive/Data_Ok/line_train_list.txt') as f:\n","  line_train_data = f.read()\n","line_train_list = eval(line_train_data)\n","line_train_set = set(line_train_list)\n","\n","with open('/content/drive/My Drive/Data_Ok/line_data.txt') as f:\n","  line_data = f.read()\n","line_set = set(eval(line_data))\n","\n","#### recover weight_list to weight_dic, ast.literal_eval can check the\n","#### list inorder not to run bad code\n","with open('/content/drive/My Drive/Data_Ok/weight_dict.txt') as weight_file:\n","  weight_list = weight_file.readline()\n","weight_dict = ast.literal_eval(weight_list)\n","\n","#### generate graph\n","G = nx.Graph()\n","weight_max = max(weight_dict.values())\n","for pair in line_train_set:\n","  G.add_edge(pair.split('#')[0], pair.split('#')[1], weight = weight_dict[pair]/weight_max)\n","for pair in line_set:\n","  G.add_node(pair.split('#')[0])\n","  G.add_node(pair.split('#')[1])\n","\n","#### Jaccard's Coefficient\n","def gen_nbr(g, pair):\n","  node_list = []\n","  weight_list = []\n","  for nbr, data in g[pair].items():\n","    node_list.append(nbr)\n","    weight_list.append(data['weight'])\n","  count = 0\n","  for num in range(len(node_list)):\n","    for num2 in range(int(weight_list[num] * weight_max)):\n","      node_list.append(node_list[num])\n","  return node_list\n","\n","def jaccard(g, pair1, pair2):\n","  node_list1 = gen_nbr(g, pair1)\n","  node_list2 = gen_nbr(g, pair2)\n","  count = 0\n","  for num1 in range(len(node_list1)):\n","    for num2 in range(len(node_list2)):\n","      if node_list1[num1] == node_list2[num2]:\n","        count += 1\n","        break\n","  if len(node_list1) == 0:\n","    return 0\n","  nbr = count / len(set.union(set(node_list1),set(node_list2)))\n","  if nbr > 0.5:\n","    return 1\n","  else:\n","    return 0\n","\n","#### use read to get strings, then use eval to recover them\n","def read_list(name, kind):\n","  with open('/content/drive/My Drive/Data_Ok/' + name + '_' + kind + '_list.txt') as f:\n","    name_kind = f.read()\n","  return eval(name_kind)\n","\n","line_test_list = read_list('line', 'test')\n","neg_test_list = read_list('neg', 'test')\n","\n","correct_num = 0 \n","for num in range(len(line_test_list)):\n","  pair1 = line_test_list[num].split('#')[0]\n","  pair2 = line_test_list[num].split('#')[1]\n","  if jaccard(G, pair1, pair2) == 1:\n","    correct_num += 1\n","for num in range(len(neg_test_list)):\n","  pair1 = neg_test_list[num].split('#')[0]\n","  pair2 = neg_test_list[num].split('#')[1]\n","  if jaccard(G, pair1, pair2) == 0:\n","    correct_num += 1\n","#line_test_set = set(line_test_list)\n","\n","score = correct_num / (len(line_test_list) + len(neg_test_list))\n","print(score)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6599734042553191\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gGiw-oKQjAMe","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1585728892950,"user_tz":-480,"elapsed":971,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"76acea84-02de-40bd-f11f-8fd4a424468c"},"source":["list1 = [1,2,3]\n","list2 = [1,2,3,4,3]\n","set2 = set(list2)\n","set1 = set(list1)\n","print(set.union(set1,set2))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{1, 2, 3, 4}\n","ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e9OJMHXw1UO-","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1588348439240,"user_tz":-480,"elapsed":61780,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"2cd2c357-ae5b-460c-a737-a349ed608982"},"source":["#### link prediction\n","\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import ast\n","\n","#### make line_data to set\n","with open('/content/drive/My Drive/Data_Ok/line_train_list.txt') as f:\n","  line_train_data = f.read()\n","line_train_list = eval(line_train_data)\n","line_train_set = set(line_train_list)\n","\n","with open('/content/drive/My Drive/Data_Ok/line_data.txt') as f:\n","  line_data = f.read()\n","line_set = set(eval(line_data))\n","\n","#### recover weight_list to weight_dic, ast.literal_eval can check the\n","#### list inorder not to run bad code\n","with open('/content/drive/My Drive/Data_Ok/weight_dict.txt') as weight_file:\n","  weight_list = weight_file.readline()\n","weight_dict = ast.literal_eval(weight_list)\n","\n","#### generate graph\n","G = nx.Graph()\n","weight_max = max(weight_dict.values())\n","for pair in line_train_set:\n","  G.add_edge(pair.split('#')[0], pair.split('#')[1], weight = weight_dict[pair]/weight_max)\n","for pair in line_set:\n","  G.add_node(pair.split('#')[0])\n","  G.add_node(pair.split('#')[1])\n","\n","#### Jaccard's Coefficient\n","def gen_nbr(g, pair):\n","  node_list = []\n","  weight_list = []\n","  for nbr, data in g[pair].items():\n","    node_list.append(nbr)\n","    weight_list.append(data['weight'])\n","  count = 0\n","  for num in range(len(node_list)):\n","    for num2 in range(int(weight_list[num] * weight_max)):\n","      node_list.append(node_list[num])\n","  return node_list\n","\n","def jaccard(g, pair1, pair2):\n","  node_list1 = gen_nbr(g, pair1)\n","  node_list2 = gen_nbr(g, pair2)\n","  count = 0\n","  for num1 in range(len(node_list1)):\n","    for num2 in range(len(node_list2)):\n","      if node_list1[num1] == node_list2[num2]:\n","        count += 1\n","        break\n","  if len(node_list1) == 0:\n","    return 0\n","#  score = count / len(set.union(set(node_list1),set(node_list2))) \n","  score = count / (len(node_list1)+len(node_list2)) \n","#  print(pair1,pair2,score)\n","  return score \n","\n","#### use read to get strings, then use eval to recover them\n","def read_list(name, kind):\n","  with open('/content/drive/My Drive/Data_Ok/' + name + '_' + kind + '_list.txt') as f:\n","    name_kind = f.read()\n","  return eval(name_kind)\n","\n","line_train_list = read_list('line', 'train')\n","neg_train_list = read_list('neg', 'train')\n","line_test_list = read_list('line', 'test')\n","neg_test_list = read_list('neg', 'test')\n","\n","def gen_data_for_learn(line_list_data, neg_list_data):\n","  x = np.zeros(len(line_list_data) * 2)\n","  y = np.zeros(len(line_list_data) * 2)\n","  for num in range(len(line_list_data)):\n","    pair1 = line_list_data[num].split('#')[0]\n","    pair2 = line_list_data[num].split('#')[1]\n","    x[num] = jaccard(G, pair1, pair2)\n","    y[num] = 1\n","  for num in range(len(neg_list_data)):\n","    pair1 = neg_list_data[num].split('#')[0]\n","    pair2 = neg_list_data[num].split('#')[1]\n","    num2 = num + len(line_list_data)\n","    x[num2] = jaccard(G, pair1, pair2)\n","  return x, y\n","  \n","train_x, train_y = gen_data_for_learn(line_train_list, neg_train_list)\n","test_x, test_y = gen_data_for_learn(line_test_list, neg_test_list)\n","\n","#### LR\n","LR_model = LogisticRegression()\n","\n","LR_model.fit(train_x.reshape(-1, 1), train_y)\n","#LR_score = accuracy_score(test_y, LR_model.predict(test_x.reshape(-1, 1)))\n","def evaluation(x, y):\n","  print('p:', precision_score(x,y),'r:', recall_score(x,y), 'f1:', f1_score(x,y))\n","\n","evaluation(test_y, LR_model.predict(test_x.reshape(-1, 1)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["p: 0.9631646380525304 r: 0.7997340425531915 f1: 0.8738738738738738\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a0UgFVB4jCx8","colab":{"base_uri":"https://localhost:8080/","height":960},"executionInfo":{"status":"ok","timestamp":1587818867704,"user_tz":-480,"elapsed":506009,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"f763c2d6-2b2b-4f08-f4ff-052754fc0aee"},"source":["#### machine learning\n","\n","from gensim.models import word2vec\n","import numpy as np\n","from sklearn.metrics import recall_score, precision_score, f1_score\n","from sklearn.linear_model import LogisticRegression \n","from sklearn.linear_model import RidgeCV\n","import sklearn.svm as svm\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","#### use read to get strings, then use eval to recover them\n","def read_list(name, kind):\n","  with open('/content/drive/My Drive/Data_Ok/' + name + '_' + kind + '_list.txt') as f:\n","    name_kind = f.read()\n","  return eval(name_kind)\n","\n","line_train_list = read_list('line', 'train')\n","neg_train_list = read_list('neg', 'train')\n","line_test_list = read_list('line', 'test')\n","neg_test_list = read_list('neg', 'test')\n","\n","def generate_xy(line_list, neg_list, x, y):\n","  i = 0\n","  for pair in line_list:\n","    x[i,0:dimentions] = w2v_model[pair.split('#')[0]]\n","    x[i,dimentions:dimentions * 2] = w2v_model[pair.split('#')[1]]\n","    y[i,0] = 1\n","    i += 1\n","  for pair in neg_list:\n","    x[i,0:dimentions] = w2v_model[pair.split('#')[0]]\n","    x[i,dimentions:dimentions * 2] = w2v_model[pair.split('#')[1]]\n","    y[i,0] = 0\n","    i += 1\n","  return x, y\n","\n","def evaluation(x, y):\n","  print('p:', precision_score(x,y),'r:', recall_score(x,y), 'f1:', f1_score(x,y))\n","\n","\n","for i in range(5):\n","  if i == 0:\n","    model = 'model11'\n","    print(model)\n","  else:\n","    model = 'model' + str(pow(1/2,i)) + str(pow(2,i))\n","    print(model)\n","\n","  w2v_model = word2vec.Word2Vec.load('/content/drive/My Drive/Result/' + model)\n","  dimentions = 64\n","\n","  train_x = np.zeros((len(line_train_list) * 2, dimentions * 2))\n","  train_y = np.zeros((len(line_train_list) * 2, 1), dtype = int)\n","  test_x = np.zeros((len(line_test_list) * 2, dimentions * 2))\n","  test_y = np.zeros((len(line_test_list) * 2, 1), dtype = int)\n","\n","\n","  train_x, train_y = generate_xy(line_train_list, neg_train_list, train_x, train_y)\n","  test_x, test_y = generate_xy(line_test_list, neg_test_list, test_x, test_y)\n","\n","#### MLPClassifier\n","  MLP_model = MLPClassifier(hidden_layer_sizes=(500, ), activation='relu', \n","    solver='lbfgs', alpha=0.0001, batch_size='auto', \n","    learning_rate='constant')\n","  MLP_model.fit(train_x, train_y)\n","#MLP_score = accuracy_score(test_y, MLP_model.predict(test_x))\n","  evaluation(test_y, MLP_model.predict(test_x))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["model11\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:934: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"stream","text":["p: 0.9858772406300923 r: 0.9654255319148937 f1: 0.9755442085460898\n","model0.52\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:934: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"stream","text":["p: 0.9820554649265906 r: 0.9606382978723405 f1: 0.9712288249529445\n","model0.254\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:934: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"stream","text":["p: 0.9809730905137266 r: 0.9598404255319148 f1: 0.9702917058744455\n","model0.1258\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:934: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"stream","text":["p: 0.9862068965517241 r: 0.9507978723404256 f1: 0.9681787406905891\n","model0.062516\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:934: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"stream","text":["p: 0.9826912339475153 r: 0.9361702127659575 f1: 0.9588667937891583\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZdqmCRZOjEx7","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1588574235226,"user_tz":-480,"elapsed":88343,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"36a1e818-b2ca-4564-da4c-b096ed468438"},"source":["#### link prediction\n","\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import recall_score, precision_score, f1_score\n","from sklearn.metrics import accuracy_score\n","import ast\n","\n","#### make line_data to set\n","with open('/content/drive/My Drive/Data_Ok/line_train_list.txt') as f:\n","  line_train_data = f.read()\n","line_train_list = eval(line_train_data)\n","line_train_set = set(line_train_list)\n","\n","with open('/content/drive/My Drive/Data_Ok/line_data.txt') as f:\n","  line_data = f.read()\n","line_set = set(eval(line_data))\n","\n","#### recover weight_list to weight_dic, ast.literal_eval can check the\n","#### list inorder not to run bad code\n","with open('/content/drive/My Drive/Data_Ok/weight_dict.txt') as weight_file:\n","  weight_list = weight_file.readline()\n","weight_dict = ast.literal_eval(weight_list)\n","\n","#### generate graph\n","G = nx.Graph()\n","weight_max = max(weight_dict.values())\n","for pair in line_train_set:\n","  G.add_edge(pair.split('#')[0], pair.split('#')[1], weight = weight_dict[pair]/weight_max)\n","for pair in line_set:\n","  G.add_node(pair.split('#')[0])\n","  G.add_node(pair.split('#')[1])\n","\n","#### Jaccard's Coefficient\n","def gen_nbr(g, pair):\n","  node_list = []\n","  weight_list = []\n","  for nbr, data in g[pair].items():\n","    node_list.append(nbr)\n","    weight_list.append(data['weight'])\n","  count = 0\n","  for num in range(len(node_list)):\n","    for num2 in range(int(weight_list[num] * weight_max)):\n","      node_list.append(node_list[num])\n","  return node_list\n","'''\n","def jaccard(g, pair1, pair2):\n","  node_list1 = gen_nbr(g, pair1)\n","  list1 = list(set(node_list1))\n","  set1 = set(node_list1)\n","  node_list2 = gen_nbr(g, pair2)\n","  list2 = list(set(node_list2))\n","  set2 = set(node_list2)\n","  count = 0\n","  for num1 in range(len(set1)):\n","    for num2 in range(len(set2)):\n","      if list1[num1] == list2[num2]:\n","        count += 1\n","        break\n","  if len(node_list1) == 0:\n","    return 0\n","  score = count / len(set.union(set1,set2)) ##jaccard\n","##  score = count / (len(node_list1)+len(node_list2)) ##jaccard + weight\n","#  print(pair1,pair2,score)#,len(list(set(node_list1))),len(node_list1))\n","  return score \n","'''\n","def jaccard(g, pair1, pair2):\n","  node_list1 = gen_nbr(g, pair1)\n","  node_list2 = gen_nbr(g, pair2)\n","  count = 0\n","  for num1 in range(len(node_list1)):\n","    for num2 in range(len(node_list2)):\n","      if node_list1[num1] == node_list2[num2]:\n","        count += 1\n","#        break\n","  if len(node_list1) == 0 or len(node_list2) == 0:\n","    return 0\n","#  score = count / len(set.union(set(node_list1),set(node_list2))) \n","  score = count / (len(node_list1) + len(node_list2)) \n","#  print(pair1,pair2,score)\n","  return score \n","\n","\n","#### use read to get strings, then use eval to recover them\n","def read_list(name, kind):\n","  with open('/content/drive/My Drive/Data_Ok/' + name + '_' + kind + '_list.txt') as f:\n","    name_kind = f.read()\n","  return eval(name_kind)\n","\n","line_train_list = read_list('line', 'train')\n","neg_train_list = read_list('neg', 'train')\n","line_test_list = read_list('line', 'test')\n","neg_test_list = read_list('neg', 'test')\n","\n","def gen_data_for_learn(line_list_data, neg_list_data):\n","  x = np.zeros(len(line_list_data) * 2)\n","  y = np.zeros(len(line_list_data) * 2)\n","  for num in range(len(line_list_data)):\n","    pair1 = line_list_data[num].split('#')[0]\n","    pair2 = line_list_data[num].split('#')[1]\n","    x[num] = jaccard(G, pair1, pair2)\n","    y[num] = 1\n","  for num in range(len(neg_list_data)):\n","    pair1 = neg_list_data[num].split('#')[0]\n","    pair2 = neg_list_data[num].split('#')[1]\n","    num2 = num + len(line_list_data)\n","    x[num2] = jaccard(G, pair1, pair2)\n","  return x, y\n","  \n","train_x, train_y = gen_data_for_learn(line_train_list, neg_train_list)\n","test_x, test_y = gen_data_for_learn(line_test_list, neg_test_list)\n","\n","#### LR\n","LR_model = LogisticRegression()\n","\n","LR_model.fit(train_x.reshape(-1, 1), train_y)\n","#LR_score = accuracy_score(test_y, LR_model.predict(test_x.reshape(-1, 1)))\n","def evaluation(x, y):\n","  print('p:', precision_score(x,y),'r:', recall_score(x,y), 'f1:', f1_score(x,y))\n","\n","evaluation(test_y, LR_model.predict(test_x.reshape(-1, 1)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["p: 0.9457399103139014 r: 0.5609042553191489 f1: 0.7041736227045075\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-pPDNQDT6vWQ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1588574137299,"user_tz":-480,"elapsed":812,"user":{"displayName":"周良军","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gijb7V9PPYGxsoPpFCZ8mOKG1m5paunlaOE4kgZ=s64","userId":"13948080381625877345"}},"outputId":"78792deb-80e2-4ead-96b1-3b3419b35bd4"},"source":["a=2\n","b=1\n","if a==2 or b==2:\n","  print(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VhSx9vUwY98y"},"source":[""],"execution_count":null,"outputs":[]}]}